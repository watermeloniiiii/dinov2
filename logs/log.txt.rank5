I20240613 14:24:28 3865241 dinov2 config.py:59] git:
  sha: e1277af2ba9496fbadf7aec6eba56e8d882d1e35, status: has uncommitted changes, branch: main

I20240613 14:24:28 3865241 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: dinov2/configs/eval/vits14_pretrain.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
local_rank: 5
no_resume: False
num_workers: 8
opts: ['train.output_dir=/NAS6/Members/linchenxi/dinov2']
output_dir: /NAS6/Members/linchenxi/dinov2
pretrained_weights: /NAS6/Members/linchenxi/dinov2/pretrained_models/dinov2_vits14_pretrain.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
train_dataset_str: ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC
val_metric_type: mean_accuracy
I20240613 14:24:28 3865241 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0028284271247461905
I20240613 14:24:28 3865241 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /NAS6/Members/linchenxi/dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_small
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0028284271247461905
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 98
evaluation:
  eval_period_iterations: 12500

I20240613 14:33:53 3867789 dinov2 config.py:59] git:
  sha: e1277af2ba9496fbadf7aec6eba56e8d882d1e35, status: has uncommitted changes, branch: main

I20240613 14:33:53 3867789 dinov2 config.py:60] batch_size: 128
classifier_fpath: None
config_file: dinov2/configs/eval/vits14_pretrain.yaml
epoch_length: 1250
epochs: 10
eval_period_iterations: 1250
learning_rates: [1e-05, 2e-05, 5e-05, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1]
local_rank: 5
no_resume: False
num_workers: 8
opts: ['train.output_dir=/NAS6/Members/linchenxi/dinov2']
output_dir: /NAS6/Members/linchenxi/dinov2
pretrained_weights: /NAS6/Members/linchenxi/dinov2/pretrained_models/dinov2_vits14_pretrain.pth
save_checkpoint_frequency: 20
test_class_mapping_fpaths: [None]
test_dataset_strs: None
test_metric_types: None
train_dataset_str: ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC
val_class_mapping_fpath: None
val_dataset_str: ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC
val_metric_type: mean_accuracy
I20240613 14:33:53 3867789 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0028284271247461905
I20240613 14:33:53 3867789 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /NAS6/Members/linchenxi/dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_small
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0028284271247461905
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 98
evaluation:
  eval_period_iterations: 12500

I20240613 14:33:53 3867789 dinov2 vision_transformer.py:126] using MLP layer as FFN
I20240613 14:33:54 3867789 dinov2 utils.py:33] Pretrained weights found at /NAS6/Members/linchenxi/dinov2/pretrained_models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>
I20240613 14:33:54 3867789 dinov2 loaders.py:84] using dataset: "ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC"
I20240613 14:33:54 3867789 dinov2 loaders.py:89] # of dataset samples: 1,281,167
I20240614 10:11:53 43741 dinov2 config.py:59] git:
  sha: e1277af2ba9496fbadf7aec6eba56e8d882d1e35, status: has uncommitted changes, branch: main

I20240614 10:11:53 43741 dinov2 config.py:60] batch_size: 256
config_file: dinov2/configs/eval/vits14_pretrain.yaml
gather_on_cpu: False
local_rank: 5
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
opts: ['train.output_dir=/NAS6/Members/linchenxi/dinov2']
output_dir: /NAS6/Members/linchenxi/dinov2
pretrained_weights: /NAS6/Members/linchenxi/dinov2/pretrained_models/dinov2_vits14_pretrain.pth
temperature: 0.07
train_dataset_str: ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC
val_dataset_str: ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC
I20240614 10:11:53 43741 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.0028284271247461905
I20240614 10:11:53 43741 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN
  output_dir: /NAS6/Members/linchenxi/dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_small
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.0028284271247461905
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 98
evaluation:
  eval_period_iterations: 12500

I20240614 10:11:53 43741 dinov2 vision_transformer.py:126] using MLP layer as FFN
I20240614 10:11:54 43741 dinov2 utils.py:33] Pretrained weights found at /NAS6/Members/linchenxi/dinov2/pretrained_models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>
I20240614 10:11:54 43741 dinov2 loaders.py:84] using dataset: "ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC"
I20240614 10:11:54 43741 dinov2 loaders.py:89] # of dataset samples: 1,281,167
I20240614 10:11:54 43741 dinov2 loaders.py:84] using dataset: "ImageNet:split=TRAIN:root=/NAS6/Members/linchenxi/ILSVRC:extra=/NAS6/Members/linchenxi/ILSVRC"
I20240614 10:11:54 43741 dinov2 loaders.py:89] # of dataset samples: 1,281,167
I20240614 10:11:54 43741 dinov2 knn.py:262] Extracting features for train set...
I20240614 10:11:54 43741 dinov2 loaders.py:147] sampler: distributed
I20240614 10:11:54 43741 dinov2 loaders.py:206] using PyTorch data loader
I20240614 10:11:54 43741 dinov2 loaders.py:219] # of batches: 626
I20240614 10:12:09 43741 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([1281167, 384])
I20240614 10:12:09 43741 dinov2 helpers.py:102]   [  0/626]  eta: 2:45:10    time: 15.830723  data: 13.774517  max mem: 2178
I20240614 10:12:25 43741 dinov2 helpers.py:102]   [ 10/626]  eta: 0:29:12    time: 2.844693  data: 2.633911  max mem: 2850
I20240614 10:12:30 43741 dinov2 helpers.py:102]   [ 20/626]  eta: 0:17:33    time: 1.033829  data: 1.008792  max mem: 2850
I20240614 10:12:36 43741 dinov2 helpers.py:102]   [ 30/626]  eta: 0:13:34    time: 0.552710  data: 0.527244  max mem: 2850
I20240614 10:12:42 43741 dinov2 helpers.py:102]   [ 40/626]  eta: 0:11:30    time: 0.591580  data: 0.565398  max mem: 2850
I20240614 10:12:48 43741 dinov2 helpers.py:102]   [ 50/626]  eta: 0:10:08    time: 0.576864  data: 0.551566  max mem: 2850
I20240614 10:12:53 43741 dinov2 helpers.py:102]   [ 60/626]  eta: 0:09:11    time: 0.557483  data: 0.532234  max mem: 2850
I20240614 10:12:59 43741 dinov2 helpers.py:102]   [ 70/626]  eta: 0:08:33    time: 0.587225  data: 0.562502  max mem: 2850
I20240614 10:13:05 43741 dinov2 helpers.py:102]   [ 80/626]  eta: 0:07:57    time: 0.571089  data: 0.547358  max mem: 2850
I20240614 10:13:11 43741 dinov2 helpers.py:102]   [ 90/626]  eta: 0:07:33    time: 0.566860  data: 0.544472  max mem: 2850
I20240614 10:13:15 43741 dinov2 helpers.py:102]   [100/626]  eta: 0:07:06    time: 0.547004  data: 0.523577  max mem: 2850
I20240614 10:13:20 43741 dinov2 helpers.py:102]   [110/626]  eta: 0:06:43    time: 0.489943  data: 0.466652  max mem: 2850
I20240614 10:13:25 43741 dinov2 helpers.py:102]   [120/626]  eta: 0:06:22    time: 0.477006  data: 0.454065  max mem: 2850
I20240614 10:13:31 43741 dinov2 helpers.py:102]   [130/626]  eta: 0:06:06    time: 0.505602  data: 0.481947  max mem: 2850
I20240614 10:13:36 43741 dinov2 helpers.py:102]   [140/626]  eta: 0:05:53    time: 0.558601  data: 0.534664  max mem: 2850
I20240614 10:13:42 43741 dinov2 helpers.py:102]   [150/626]  eta: 0:05:40    time: 0.552167  data: 0.525793  max mem: 2850
I20240614 10:13:47 43741 dinov2 helpers.py:102]   [160/626]  eta: 0:05:27    time: 0.525359  data: 0.498281  max mem: 2850
I20240614 10:13:52 43741 dinov2 helpers.py:102]   [170/626]  eta: 0:05:15    time: 0.522368  data: 0.495311  max mem: 2850
I20240614 10:13:58 43741 dinov2 helpers.py:102]   [180/626]  eta: 0:05:05    time: 0.544217  data: 0.516548  max mem: 2850
I20240614 10:14:03 43741 dinov2 helpers.py:102]   [190/626]  eta: 0:04:55    time: 0.548465  data: 0.521627  max mem: 2850
I20240614 10:14:09 43741 dinov2 helpers.py:102]   [200/626]  eta: 0:04:47    time: 0.588746  data: 0.562001  max mem: 2850
I20240614 10:14:15 43741 dinov2 helpers.py:102]   [210/626]  eta: 0:04:39    time: 0.617910  data: 0.590232  max mem: 2850
I20240614 10:14:20 43741 dinov2 helpers.py:102]   [220/626]  eta: 0:04:29    time: 0.551173  data: 0.521284  max mem: 2850
I20240614 10:14:25 43741 dinov2 helpers.py:102]   [230/626]  eta: 0:04:20    time: 0.502994  data: 0.475731  max mem: 2850
I20240614 10:14:31 43741 dinov2 helpers.py:102]   [240/626]  eta: 0:04:12    time: 0.548104  data: 0.524474  max mem: 2850
I20240614 10:14:37 43741 dinov2 helpers.py:102]   [250/626]  eta: 0:04:04    time: 0.585585  data: 0.561539  max mem: 2850
I20240614 10:14:43 43741 dinov2 helpers.py:102]   [260/626]  eta: 0:03:57    time: 0.585014  data: 0.558907  max mem: 2850
I20240614 10:14:49 43741 dinov2 helpers.py:102]   [270/626]  eta: 0:03:50    time: 0.596724  data: 0.568420  max mem: 2850
I20240614 10:14:55 43741 dinov2 helpers.py:102]   [280/626]  eta: 0:03:43    time: 0.596265  data: 0.570729  max mem: 2850
I20240614 10:15:00 43741 dinov2 helpers.py:102]   [290/626]  eta: 0:03:35    time: 0.560224  data: 0.536188  max mem: 2850
I20240614 10:15:06 43741 dinov2 helpers.py:102]   [300/626]  eta: 0:03:28    time: 0.558038  data: 0.533249  max mem: 2850
I20240614 10:15:12 43741 dinov2 helpers.py:102]   [310/626]  eta: 0:03:21    time: 0.587049  data: 0.562079  max mem: 2850
I20240614 10:15:18 43741 dinov2 helpers.py:102]   [320/626]  eta: 0:03:14    time: 0.573056  data: 0.548862  max mem: 2850
I20240614 10:15:24 43741 dinov2 helpers.py:102]   [330/626]  eta: 0:03:07    time: 0.588012  data: 0.562315  max mem: 2850
I20240614 10:15:29 43741 dinov2 helpers.py:102]   [340/626]  eta: 0:03:00    time: 0.576515  data: 0.549675  max mem: 2850
I20240614 10:15:35 43741 dinov2 helpers.py:102]   [350/626]  eta: 0:02:54    time: 0.572665  data: 0.547078  max mem: 2850
I20240614 10:15:42 43741 dinov2 helpers.py:102]   [360/626]  eta: 0:02:48    time: 0.648184  data: 0.625152  max mem: 2850
I20240614 10:15:48 43741 dinov2 helpers.py:102]   [370/626]  eta: 0:02:41    time: 0.633725  data: 0.611118  max mem: 2850
I20240614 10:15:53 43741 dinov2 helpers.py:102]   [380/626]  eta: 0:02:34    time: 0.560155  data: 0.536294  max mem: 2850
I20240614 10:15:59 43741 dinov2 helpers.py:102]   [390/626]  eta: 0:02:28    time: 0.561878  data: 0.540038  max mem: 2850
I20240614 10:16:04 43741 dinov2 helpers.py:102]   [400/626]  eta: 0:02:21    time: 0.549012  data: 0.526885  max mem: 2850
I20240614 10:16:10 43741 dinov2 helpers.py:102]   [410/626]  eta: 0:02:14    time: 0.563797  data: 0.541112  max mem: 2850
I20240614 10:16:16 43741 dinov2 helpers.py:102]   [420/626]  eta: 0:02:08    time: 0.598433  data: 0.575260  max mem: 2850
I20240614 10:16:22 43741 dinov2 helpers.py:102]   [430/626]  eta: 0:02:02    time: 0.585538  data: 0.560800  max mem: 2850
I20240614 10:16:28 43741 dinov2 helpers.py:102]   [440/626]  eta: 0:01:55    time: 0.602001  data: 0.577005  max mem: 2850
I20240614 10:16:34 43741 dinov2 helpers.py:102]   [450/626]  eta: 0:01:49    time: 0.576813  data: 0.550722  max mem: 2850
I20240614 10:16:39 43741 dinov2 helpers.py:102]   [460/626]  eta: 0:01:42    time: 0.545687  data: 0.517682  max mem: 2850
I20240614 10:16:46 43741 dinov2 helpers.py:102]   [470/626]  eta: 0:01:36    time: 0.601078  data: 0.543226  max mem: 2850
I20240614 10:16:52 43741 dinov2 helpers.py:102]   [480/626]  eta: 0:01:30    time: 0.638836  data: 0.528667  max mem: 2850
I20240614 10:16:58 43741 dinov2 helpers.py:102]   [490/626]  eta: 0:01:24    time: 0.592594  data: 0.495618  max mem: 2850
I20240614 10:17:04 43741 dinov2 helpers.py:102]   [500/626]  eta: 0:01:18    time: 0.592243  data: 0.549912  max mem: 2850
I20240614 10:17:10 43741 dinov2 helpers.py:102]   [510/626]  eta: 0:01:11    time: 0.606381  data: 0.580065  max mem: 2850
I20240614 10:17:15 43741 dinov2 helpers.py:102]   [520/626]  eta: 0:01:05    time: 0.559682  data: 0.526827  max mem: 2850
I20240723 23:31:35 3558503 dinov2 config.py:59] git:
  sha: e1277af2ba9496fbadf7aec6eba56e8d882d1e35, status: has uncommitted changes, branch: main

I20240723 23:31:35 3558503 dinov2 config.py:60] batch_size: 256
config_file: /NAS6/Members/linchenxi/projects/DINOV2/model8/config.yaml
gather_on_cpu: False
local_rank: 5
n_per_class_list: [-1]
n_tries: 1
nb_knn: [10, 20, 100, 200]
opts: ['train.output_dir=/NAS6/Members/linchenxi/dinov2']
output_dir: /NAS6/Members/linchenxi/dinov2
pretrained_weights: /NAS6/Members/linchenxi/projects/DINOV2/model8/eval/training_24999/teacher_checkpoint.pth
temperature: 0.07
train_dataset_str: ClusterSentinel2:split=TRAIN:root=/NAS6/Members/linchenxi/projects/RS_foundation_model/satlas/clusters:extra=/NAS6/Members/linchenxi/ILSVRC
val_dataset_str: ClusterSentinel2:split=TEST:root=/NAS6/Members/linchenxi/projects/RS_foundation_model/satlas/clusters:extra=/NAS6/Members/linchenxi/ILSVRC
I20240723 23:31:35 3558503 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.002
I20240723 23:31:35 3558503 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.3
  mask_ratio_min_max:
  - 0.1
  - 0.3
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 32
  dataset_path: Sentinel2:split=TRAIN:root=/NAS6/Members/linchenxi/projects/RS_foundation_model/satlas:extra=/NAS6/Members/linchenxi/ILSVRC
  output_dir: /NAS6/Members/linchenxi/dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 5
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  in_chans: 9
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 2
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.002
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 368
  local_crops_size: 192
evaluation:
  eval_period_iterations: 12500

I20240723 23:31:35 3558503 dinov2 vision_transformer.py:126] using MLP layer as FFN
I20240723 23:31:37 3558503 dinov2 utils.py:26] Take key teacher in provided checkpoint dict
I20240723 23:31:37 3558503 dinov2 utils.py:33] Pretrained weights found at /NAS6/Members/linchenxi/projects/DINOV2/model8/eval/training_24999/teacher_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])
I20240723 23:31:37 3558503 dinov2 loaders.py:92] using dataset: "ClusterSentinel2:split=TRAIN:root=/NAS6/Members/linchenxi/projects/RS_foundation_model/satlas/clusters:extra=/NAS6/Members/linchenxi/ILSVRC"
I20240723 23:31:38 3558503 dinov2 loaders.py:97] # of dataset samples: 164,544
I20240723 23:31:38 3558503 dinov2 loaders.py:92] using dataset: "ClusterSentinel2:split=TEST:root=/NAS6/Members/linchenxi/projects/RS_foundation_model/satlas/clusters:extra=/NAS6/Members/linchenxi/ILSVRC"
I20240723 23:31:39 3558503 dinov2 loaders.py:97] # of dataset samples: 17,563
I20240723 23:31:39 3558503 dinov2 knn.py:262] Extracting features for train set...
I20240723 23:31:39 3558503 dinov2 loaders.py:155] sampler: distributed
I20240723 23:31:39 3558503 dinov2 loaders.py:214] using PyTorch data loader
I20240723 23:31:39 3558503 dinov2 loaders.py:227] # of batches: 81
W20240723 23:31:40 3559003 py.warnings warnings.py:109] /NAS6/Members/linchenxi/dinov2/dinov2_venv/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.
  warnings.warn(

W20240723 23:31:42 3559011 py.warnings warnings.py:109] /NAS6/Members/linchenxi/dinov2/dinov2_venv/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.
  warnings.warn(

W20240723 23:31:46 3559024 py.warnings warnings.py:109] /NAS6/Members/linchenxi/dinov2/dinov2_venv/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.
  warnings.warn(

W20240723 23:31:51 3559044 py.warnings warnings.py:109] /NAS6/Members/linchenxi/dinov2/dinov2_venv/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.
  warnings.warn(

W20240723 23:31:57 3559065 py.warnings warnings.py:109] /NAS6/Members/linchenxi/dinov2/dinov2_venv/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.
  warnings.warn(

I20240723 23:32:16 3558503 dinov2 utils.py:129] Storing features into tensor of shape torch.Size([164544, 768])
I20240723 23:32:16 3558503 dinov2 helpers.py:103]   [ 0/81]  eta: 0:50:17    time: 37.253780  data: 23.528854  max mem: 8979
